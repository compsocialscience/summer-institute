{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbkj_EAxFGQQ"
      },
      "source": [
        "# Example: Emotion Feature Extraction With Mexca\n",
        "\n",
        "In this notebook, we will use the US presidential debate between Clinton and Trump in 2016 as an example to capture and compare emotion expressions with MEXCA. The video can be found on [YouTube](https://www.youtube.com/watch?v=DBhrSdjePkk), but we will use a file that is hosted by a third party.\n",
        "\n",
        "The video contains three persons (Hillary Clinton, Donald Trump, and a moderator), but we will use a part of the video in this example where only Clinton and Trump are present. Most frames of the video contain a least one face and speech. Because it contains only a limited number of faces and speakers and the faces are mostly shown in close-ups, the video is a good example to demonstrate MEXCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPnyRAFFgmtv"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "It is recommended to run MEXCA on a GPU, so we need to make sure that a GPU is available. In Google Colab, we can do this by chaning the runtime type (under `Runtime` select `Change runtime type` and `T4 GPU`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6VFDpJRYGES"
      },
      "source": [
        "Before we begin, we must install the full version of the mexca Python package. This can be done with `pip install mexca[all]`. The `[all]` appendix indicates that all components of the MEXCA pipeline should be installed. The installation can take a few minutes to finish. (Note that `!` is an IPython magic command to run a line as a shell command)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO9oURzDYKPA"
      },
      "outputs": [],
      "source": [
        "!pip install mexca[all]\n",
        "\n",
        "# Fixes a bug with Colab and triton package\n",
        "!pip install --no-deps \"triton==2.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td6WT9AoV_0S"
      },
      "source": [
        "To check if the installation was successful, we can try to access the version of the installed mexca package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzVEMYuSWZOE"
      },
      "outputs": [],
      "source": [
        "import mexca\n",
        "\n",
        "mexca.__version__ # Should return `1.0.4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwS_kIzt0Ynk"
      },
      "source": [
        "We can check if a GPU is available using `torch.cuda.is_available()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HLhlfFyTqwR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available() # Should return `True` if GPU is available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxO0-C4IZKvq"
      },
      "source": [
        "Let us now import the required packages for the remainder of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vF37KDLFGQS"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "import yaml\n",
        "\n",
        "from base64 import b64encode\n",
        "from google.colab import userdata\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from mexca.audio import SpeakerIdentifier, VoiceExtractor\n",
        "from mexca.pipeline import Pipeline\n",
        "from mexca.text import AudioTranscriber, SentimentExtractor\n",
        "from mexca.video import FaceExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99A2l0OFGQT"
      },
      "source": [
        "We also need to download the example video file from the third party URL. First, we define a function to download a file from an URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTGDNjuEFGQT"
      },
      "outputs": [],
      "source": [
        "def download_example(url, filename):\n",
        "    # Check if filename exists\n",
        "    if not os.path.exists(filename):\n",
        "        video = urlopen(url)\n",
        "\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(video.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToAgA878aXCx"
      },
      "source": [
        "Then, we specify the URL, a name for the video file, and use the download function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGMynhY8FGQT"
      },
      "outputs": [],
      "source": [
        "video_url = 'https://books.psychstat.org/rdata/data/debate.mp4'\n",
        "filename = 'debate.mp4'\n",
        "\n",
        "download_example(video_url, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Ua0j63d6BZ"
      },
      "source": [
        " We can run `os.path.exists()` to check if the video was successfully downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXSldZ_4d_4R"
      },
      "outputs": [],
      "source": [
        "os.path.exists(filename) # Should return `True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpZxsvXxFGQU"
      },
      "source": [
        "## Building the Pipeline\n",
        "\n",
        "Next, we build the pipeline by combining different components for video, audio, and text processing. We first define the number of persons shown in the part of the video that we will analyze (Clinton and Trump). Setting the number of faces and speakers correctly is important as emotion expression features might be attributed to the wrong person otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8RpHzn1xIG2"
      },
      "outputs": [],
      "source": [
        "num_clusters = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXeFq0k_xIuk"
      },
      "source": [
        "Then, we set the device on which the pipeline components should be run. If a GPU is available, we will use it. Otherwise, we will run the components on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIYFMdBtxS8c"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    torch.device(type=\"cuda\")\n",
        "    if torch.cuda.is_available()\n",
        "    else torch.device(type=\"cpu\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqVMtpP8ww9k"
      },
      "source": [
        "To detect and extract features from faces shown in the video, we create the `FaceExtractor` component. We set `num_faces=num_clusters` so that detected faces will be assigned to two clusters based on their encoded representations (embeddings). We also add that the component should run on our specified device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHTS6VIzFGQU"
      },
      "outputs": [],
      "source": [
        "face_extractor = FaceExtractor(\n",
        "    num_faces=num_clusters,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JM4KCacFGQU"
      },
      "source": [
        "For the audio processing, we create two components: The `SpeakerIdentifier` detects speech segmenets in the audio signal and assigns them to speaker clusters. As with the faces, we assume that the video has two speakers, so we set `num_speakers=num_clusters`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmWSPYBHFGQV"
      },
      "source": [
        "*Note*: mexca builds on pretrained models from the pyannote.audio package. Since release 2.1.1, downloading the pretrained models requires the user to accept two user agreements on Hugging Face hub and generate an authentication token. Therefore, to run the mexca pipeline, please accept the user agreements on [here](https://huggingface.co/pyannote/speaker-diarization) and [here](https://huggingface.co/pyannote/segmentation). Then, generate an authentication token [here](https://huggingface.co/settings/tokens). Use this token as the value for `use_auth_token` (instead of `\"HF_TOKEN\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHw5ERBTO8Z_"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    raise Exception(\"Please generate your own access token for Hugging Face Hub\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bNHBDb_FGQV"
      },
      "outputs": [],
      "source": [
        "speaker_identifier = SpeakerIdentifier(\n",
        "    num_speakers=num_clusters,\n",
        "    device=device,\n",
        "    use_auth_token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSjN7YjFGQV"
      },
      "source": [
        "The `VoiceExtractor` computes vocal emotion expression features from the audio stream of the video. The configuration of the extracted voice feature set can be changed by setting `config=mexca.data.VoiceFeaturesConfig()`, but we will keep the default configuration for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoTSFQe9FGQV"
      },
      "outputs": [],
      "source": [
        "voice_extractor = VoiceExtractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJvMMtPrFGQV"
      },
      "source": [
        "To extract the sentiment from the spoken text, we create two text processing components. First, we transcribe the audio signal to text using the `AudioTranscriber` class. The component automatically detects the spoken language of each speech segment. The transcribed text is split into single sentences. The transcription is done using a Whisper model which comes in different sizes. Larger sized models make in most cases more accurate transcriptions but take longer to run. We set the size of the model with `whisper_model=\"medium\"` to use a medium sized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc17UQf1FGQW"
      },
      "outputs": [],
      "source": [
        "audio_transcriber = AudioTranscriber(\n",
        "    whisper_model=\"medium\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQPQTB2RFGQW"
      },
      "source": [
        "Second, the `SentimentExtractor` predicts a positive, negative, and neutral sentiment score for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy16BzeMFGQW"
      },
      "outputs": [],
      "source": [
        "sentiment_extractor = SentimentExtractor(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwOTBEOFGQW"
      },
      "source": [
        "Now, we combine the five components into a `Pipeline` instance, which will run them after each other and integrate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmQmgvETFGQW"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(\n",
        "    face_extractor=face_extractor,\n",
        "    speaker_identifier=speaker_identifier,\n",
        "    voice_extractor=voice_extractor,\n",
        "    audio_transcriber=audio_transcriber,\n",
        "    sentiment_extractor=sentiment_extractor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGYN-znFGQW"
      },
      "source": [
        "To track the progress of the pipeline, we create a logger to print messages at the `INFO` level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHKIFSEUAAFX"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=\"INFO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGADjklcFGQW"
      },
      "source": [
        "To run the pipeline, we call the `apply()` method. The video has a frame rate of 25 and to speed up the processing, we choose to process 10 video frames at a time (`frame_batch_size=10`) and to only process every 5th frame (`skip_frames=5`), assuming that emotion expressions do not change substantially faster than 200ms. For this example, we also indicate to only process the first 30 seconds using `process_subclip=(0, 30)`.\n",
        "\n",
        "**Note**: The first time you run the pipeline pre-trained models will be automatically downloaded. That and running MEXCA can take a few minutes. In the next section, we will use existing results for the entire video, so we do not need to execute the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqJXDGtwFGQW"
      },
      "outputs": [],
      "source": [
        "output = pipeline.apply(\n",
        "    filename,\n",
        "    frame_batch_size=10,\n",
        "    skip_frames=5,\n",
        "    process_subclip=(0, 30)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeJ5JVjKfqlZ"
      },
      "source": [
        "## Analyzing Multimodal Emotion Expressions\n",
        "\n",
        "Instead of only using the output for the first 30 seconds, we download existing results for the entire video from Hugging Face Hub. We take a look and the first rows of the output data which is a `polars.DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WmITq6Rfqlc"
      },
      "outputs": [],
      "source": [
        "repo_id = \"mexca/comptext-workshop-debate-full\"\n",
        "filename = \"debate_full.csv\"\n",
        "\n",
        "output_df = pl.read_csv(\n",
        "    hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
        ")\n",
        "output_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVqx6lgAFGQX"
      },
      "source": [
        "### Analyzing Facial Expressions\n",
        "\n",
        "We start analyzing the output by comparing facial action unit activations between Clinton and Trump."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIpXavfWFGQX"
      },
      "outputs": [],
      "source": [
        "def stderr(x):\n",
        "    \"\"\"Calculate the standard error of the mean\n",
        "    \"\"\"\n",
        "    return np.std(x)/np.sqrt(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aAu3j4YFGQX"
      },
      "outputs": [],
      "source": [
        "clinton_id = 0\n",
        "trump_id = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Y8Gy6Ufqld"
      },
      "outputs": [],
      "source": [
        "n_au = 27 # Nr of action units\n",
        "\n",
        "# Convert face AU features into long format\n",
        "au_df = (\n",
        "    output_df\n",
        "    .select(\n",
        "        pl.col(\"face_label\"),\n",
        "        cs.contains(\"face_au\")\n",
        "    )\n",
        "    .filter(pl.col(\"face_label\").is_not_null())\n",
        "    .melt(id_vars=\"face_label\")\n",
        "    .with_columns(\n",
        "        pl.col(\"variable\").str.extract(r\"(\\d+)\").alias(\"face_au\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compute mean and standard error for each action unit\n",
        "au_stats = (\n",
        "    au_df\n",
        "    .groupby('face_label', \"face_au\")\n",
        "    .agg(\n",
        "        pl.mean(\"value\").alias(\"avg\"),\n",
        "        (pl.std(\"value\")/pl.count().sqrt()).alias(\"ste\")\n",
        "    )\n",
        "    .sort(\"face_au\", \"face_label\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldwqyOZaFGQX"
      },
      "outputs": [],
      "source": [
        "# Reference ids of the action units\n",
        "au_ref = [1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22,\n",
        "          23, 24, 25, 26, 27, 32, 38, 39]\n",
        "\n",
        "aus = np.arange(n_au)\n",
        "\n",
        "# Create bar plot with error bars\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "clinton_au_df = au_stats.filter(\n",
        "    pl.col(\"face_label\") == clinton_id\n",
        ")\n",
        "trump_au_df = au_stats.filter(\n",
        "    pl.col(\"face_label\") == trump_id\n",
        ")\n",
        "\n",
        "ax.bar(aus-width/2, clinton_au_df.select(pl.col(\"avg\")).to_series(), width,\n",
        "       yerr=1.96*clinton_au_df.select(pl.col(\"ste\")).to_series(),\n",
        "       capsize=4, ecolor='darkgray', label='Clinton', color='seagreen')\n",
        "\n",
        "ax.bar(aus+width/2, trump_au_df.select(pl.col(\"avg\")).to_series(), width,\n",
        "       yerr=1.96*trump_au_df.select(pl.col(\"ste\")).to_series(),\n",
        "       capsize=4, ecolor='darkgray', label='Trump', color='indianred')\n",
        "\n",
        "ax.set_xlabel('Action unit')\n",
        "ax.set_xticks(aus, au_ref)\n",
        "ax.set_ylabel('Mean activation (95% CI)')\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL020WOgFGQX"
      },
      "source": [
        "The FaceExtractor compoenent extracts the activations of 41 action units. Here we only select the first 27 which are bilateral units (the following 14 units correspond to left and right unilateral activations). The bar plot shows substantial differences between Clinton and Trump in the mean activations of units associated with joy (6 and 12; Trump higher than Clinton). There are also differences in units related to sadness (1 and 4; Trump higher than Clinton). For other discrete emotions, the patterns are not as clear (e.g., for anger, Clinton has a lower score on 4 but a higher score on 5). Note that these results must interpreted with care, as we are not comparing the activations against a reference data base or baseline.\n",
        "\n",
        "### Analyzing the Voice\n",
        "\n",
        "Besides facial emotion expressions, mexca also allows us to analyze vocal expressions. By default, it extracts the voice pitch measured as the fundamental frequency F0 from speakers in the video which indicates emphasis and is related to emotional arousal. Similar to the action units, we can compare voice pitch between Clinton and Trump. For an overview of all voice features, see the [documentation](https://mexca.readthedocs.io/en/latest/output.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJKBo9p6D5mv"
      },
      "outputs": [],
      "source": [
        "# Get speaker IDs, pitch and time colulmns\n",
        "segment_speaker, pitch, time = output_df.select(\n",
        "    pl.col(\"segment_speaker_label\", \"pitch_f0_hz\", \"time\")\n",
        ").to_numpy().T\n",
        "\n",
        "# Set non-speaker frames to NaN to avoid lines connecting separate speech segments\n",
        "clinton_time = time.copy()\n",
        "clinton_time[segment_speaker != clinton_id] = np.nan\n",
        "clinton_pitch = pitch.copy()\n",
        "clinton_pitch[segment_speaker != clinton_id] = np.nan\n",
        "\n",
        "trump_time = time.copy()\n",
        "trump_time[segment_speaker != trump_id] = np.nan\n",
        "trump_pitch = pitch.copy()\n",
        "trump_pitch[segment_speaker != trump_id] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snQFzk3JFGQX"
      },
      "outputs": [],
      "source": [
        "# Create line plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(clinton_time, clinton_pitch, label='Clinton', color='seagreen')\n",
        "ax.plot(clinton_time, [0] * clinton_time.shape[0], color = 'seagreen')\n",
        "ax.axhline(np.nanmean(clinton_pitch), ls='--', color='seagreen')\n",
        "\n",
        "ax.plot(trump_time, trump_pitch, label='Trump', color='indianred')\n",
        "ax.plot(trump_time, [-5] * trump_time.shape[0], color = 'indianred')\n",
        "ax.axhline(np.nanmean(trump_pitch), ls='--', color='indianred')\n",
        "\n",
        "ax.set_xlabel('Time (in s)')\n",
        "ax.set_ylabel('Pitch (F0 in Hz)')\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U01AvGAkFGQY"
      },
      "source": [
        "The figure shows the voice pitch of Clinton and Trump over time and displays the mean pitch (dashed line). It shows that the baseline pitch of Clintons's voice is higher on average than Trump's which is expceted since female speakers tend to have a higher average pitch than male speakers. We can also see that the pitch is higher for both candidates in the first half of the debate than in the second. Voice pitch is associated with emotional arousal suggesting increased display in the first half.\n",
        "\n",
        "### Analyzing the Text\n",
        "\n",
        "Next to facial expressions and voice features, mexca can also extract the sentiment from the spoken text. Again, we can compare the positive, negative, and neutral sentiment in the speech content between Clinton and Trump."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LguW4XiMJI3F"
      },
      "outputs": [],
      "source": [
        "# Extract text sentiment\n",
        "sent_pos, sent_neg, sent_neu = output_df.select(\n",
        "    pl.col(\"span_sent_pos\", \"span_sent_neg\", \"span_sent_neu\")\n",
        ").to_numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JObzFlHWFGQY"
      },
      "outputs": [],
      "source": [
        "# Create line plot\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
        "\n",
        "ax1.plot(clinton_time, sent_pos, label='Clinton', color='seagreen')\n",
        "ax1.plot(trump_time, sent_pos, label='Trump', color='indianred')\n",
        "ax2.plot(clinton_time, sent_neg, label='Clinton', color='seagreen')\n",
        "ax2.plot(trump_time, sent_neg, label='Trump', color='indianred')\n",
        "ax3.plot(clinton_time, sent_neu, label='Clinton', color='seagreen')\n",
        "ax3.plot(trump_time, sent_neu, label='Trump', color='indianred')\n",
        "\n",
        "ax1.set_title('Positive')\n",
        "ax2.set_title('Negative')\n",
        "ax3.set_title('Neutral')\n",
        "ax3.set_xlabel('Time (in s)')\n",
        "for ax in (ax1, ax2, ax3):\n",
        "    # ax.set_xticks(np.arange(35, step=5.0))\n",
        "    ax.set_yticks(np.arange(1.2, step=0.2))\n",
        "ax2.set_ylabel('Sentiment score')\n",
        "ax2.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX90jvqNFGQY"
      },
      "source": [
        "Overall, we can see that Trump has much more variation in the sentiment including sharp jumps from negative to positive or vice versa. He has a very strong negative and positive peak between seconds 150 and 160. We can look at the specific sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHw5kJetJ5d-"
      },
      "outputs": [],
      "source": [
        "# Print transcription at peak time window\n",
        "with pl.Config(fmt_str_lengths=100):\n",
        "    print((output_df\n",
        "        .filter(\n",
        "            pl.col(\"time\").is_between(150, 160) &\n",
        "            pl.col(\"span_text\").is_not_null()\n",
        "        )\n",
        "        .select(pl.col(\"time\", \"span_text\"))\n",
        "        .unique(subset=\"span_text\")\n",
        "        .sort(\"time\")\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deDU8Lx6fqle"
      },
      "source": [
        "The phrase \"terrible disrespect\" is predicted as being extremely negative and the words \"lovely\" and \"wonderful\" as extremely positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydFpb80KFGQY"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this example, we build a custom pipeline using the mexca package to extract emotion expressions from a video. We ran the pipeline on an excerpt from the US presidential debate 2016 between Clinton and Trump. We analyzed differences in facial action unit activations, voice pitch, and speech text sentiment between the two candidates.\n",
        "\n",
        "## References\n",
        "\n",
        "LÃ¼ken, M., Moodley, K., Viviani, E., Pipal, C., & Schumacher, G. (2024, January 18). MEXCA - A simple and robust pipeline for capturing emotion expressions in faces, vocalization, and speech. https://doi.org/10.31234/osf.io/56svb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mexca",
      "language": "python",
      "name": "mexca"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "cf45cf28e02693e1f07e3287e6807361380631608e21e3f53ef1041bfccc5ce4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}