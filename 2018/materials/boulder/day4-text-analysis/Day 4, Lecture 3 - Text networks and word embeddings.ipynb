{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from disk into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('potus_wiki_bios_cleaned.json','r') as f:\n",
    "    bios = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm there are 44 presidents (shaking fist at [Grover Cleveland](https://en.wikipedia.org/wiki/Grover_Cleveland)) in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {0} biographies of presidents.\".format(len(bios)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's an example of a single biography? We access the dictionary by passing the key (President's name), which returns the value (the text of the biography)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = bios['Grover Cleveland']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some metadata about the U.S. Presidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents_df = pd.DataFrame(requests.get('https://raw.githubusercontent.com/hitch17/sample-data/master/presidents.json').json())\n",
    "presidents_df = presidents_df.set_index('president')\n",
    "presidents_df['wikibio words'] = pd.Series({bio_name:len(bio_text) for bio_name,bio_text in bios.items()})\n",
    "presidents_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really basic exploratory scatterplot for the number of words in each President's biography compared to their POTUS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents_df.plot.scatter(x='number',y='wikibio words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "We can create a document-term matrix where the rows are our 44 presidential biographies, the columns are the terms (words), and the values in the cells are the word counts: the number of times that document contains that word. This is the \"term frequency\" (TF) part of TF-IDF.\n",
    "\n",
    "The IDF part of TF-IDF is the \"inverse document frequency\". The intuition is that words that occur frequency within a single document but are infrequent across the corpus of documents should receiving a higher weighting: these words have greater relative meaning. Conversely, words that are frequently used across documents are down-weighted.\n",
    "\n",
    "The image below has documents as columns and terms as rows.\n",
    "\n",
    "![Document-term matrix](http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Compute the word counts -- it expects a big string, so join our cleaned words back together\n",
    "bio_counts = count_vect.fit_transform([' '.join(bio) for bio in bios.values()])\n",
    "\n",
    "# Compute the TF-IDF for the word counts from each biography\n",
    "bio_tfidf = TfidfTransformer().fit_transform(bio_counts)\n",
    "\n",
    "# Convert from sparse to dense array representation\n",
    "bio_tfidf_dense = bio_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a text similarity network\n",
    "\n",
    "Once we have the TFIDF scores for every word in each president's biography, we can make a text similarity network. Multiplying the document by term matrix by its transpose should return the [cosine similarities](https://en.wikipedia.org/wiki/Cosine_similarity) between documents. We can also import [`cosine_similarity`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) from scikit-learn if you don't believe me (I didn't believe me either). Cosine similarity values closer to 1 indicate these documents' words have more similar TFIDF scores and values closer to 0 indicate these documents' words are more dissimilar.\n",
    "\n",
    "The goal here is to create a network where nodes are presidents and edges are weighted similarity scores. All text documents will have some minimal similarity, so we can threshold the similarity scores to only those similarities in the top 10% for each president. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "pres_pres_df = pd.DataFrame(bio_tfidf_dense*bio_tfidf_dense.T)\n",
    "\n",
    "# If you don't believe me that cosine similiarty is the document-term matrix times its transpose\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "pres_pres_df = pd.DataFrame(cosine_similarity(bio_tfidf_dense))\n",
    "\n",
    "# Filter for edges in the 90th percentile or greater\n",
    "pres_pres_filtered_df = pres_pres_df[pres_pres_df >= pres_pres_df.quantile(.9)]\n",
    "\n",
    "# Reshape and filter data\n",
    "edgelist_df = pres_pres_filtered_df.stack().reset_index()\n",
    "edgelist_df = edgelist_df[(edgelist_df[0] != 0) & (edgelist_df['level_0'] != edgelist_df['level_1'])]\n",
    "\n",
    "# Rename and replace data\n",
    "edgelist_df.rename(columns={'level_0':'from','level_1':'to',0:'weight'},inplace=True)\n",
    "edgelist_df.replace(dict(enumerate(bios.keys())),inplace=True)\n",
    "\n",
    "# Inspect\n",
    "edgelist_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read this pandas edgelist into networkx using `from_pandas_edgelist`, report out some basic descriptives about the network, and write the graph object to file in case we want to visualize it in a dedicated network visualization package like [Gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from edgelist to a graph object\n",
    "g = nx.from_pandas_edgelist(edgelist_df,source='from',target='to',edge_attr=['weight'])\n",
    "\n",
    "# Report out basic descriptives\n",
    "print(\"There are {0:,} nodes and {1:,} edges in the network.\".format(g.number_of_nodes(),g.number_of_edges()))\n",
    "\n",
    "# Write graph object to disk for visualization\n",
    "nx.write_gexf(g,'bio_similarity.gexf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small and sparse network, we can try to use Matplotlib to visualize it instead. I would only use the `nx.draw` functionality for small networks like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the nodes as a spring layout\n",
    "\n",
    "#g_pos = nx.layout.fruchterman_reingold_layout(g, k = 5, iterations=10000)\n",
    "g_pos = nx.layout.kamada_kawai_layout(g)\n",
    "\n",
    "# Draw the graph\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "nx.draw(G = g,\n",
    "        ax = ax,\n",
    "        pos = g_pos,\n",
    "        with_labels = True,\n",
    "        node_size = [dc*(len(g) - 1)*100 for dc in nx.degree_centrality(g).values()],\n",
    "        font_size = 10,\n",
    "        font_weight = 'bold',\n",
    "        width = [d['weight']*10 for i,j,d in g.edges(data=True)],\n",
    "        node_color = 'tomato',\n",
    "        edge_color = 'grey'\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Text similarity network of the S&P 500 companies\n",
    "\n",
    "**Step 1**: Load and preprocess the content of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('sp500_wiki_articles.json','r') as f:\n",
    "    sp500_articles = json.load(f)\n",
    "\n",
    "# Bring in the text_preprocessor we wrote from Day 4, Lecture 1\n",
    "def text_preprocessor(text):\n",
    "    \"\"\"Takes a large string (document) and returns a list of cleaned tokens\"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    clean_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.lower() not in all_stopwords and len(t) > 2:\n",
    "            clean_tokens.append(lemmatizer(t.lower()))\n",
    "    return clean_tokens\n",
    "\n",
    "# Clean each article\n",
    "cleaned_sp500 = {}\n",
    "\n",
    "for name,text in sp500_articles.items():\n",
    "    cleaned_sp500[name] = text_preprocessor(text)\n",
    "\n",
    "# Save to disk\n",
    "with open('sp500_wiki_articles_cleaned.json','w') as f:\n",
    "    json.dump(cleaned_sp500,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Compute the TFIDF matrix for the S&P 500 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the word counts\n",
    "sp500_counts = \n",
    "\n",
    "# Compute the TF-IDF for the word counts from each biography\n",
    "sp500_tfidf = \n",
    "\n",
    "# Convert from sparse to dense array representation\n",
    "sp500_tfidf_dense = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Compute the cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "company_company_df = \n",
    "\n",
    "# Filter for edges in the 90th percentile or greater\n",
    "company_company_filtered_df = \n",
    "\n",
    "# Reshape and filter data\n",
    "sp500_edgelist_df = \n",
    "sp500_edgelist_df = \n",
    "\n",
    "# Rename and replace data\n",
    "sp500_edgelist_df.rename(columns={'level_0':'from','level_1':'to',0:'weight'},inplace=True)\n",
    "sp500_edgelist_df.replace(dict(enumerate(sp500_articles.keys())),inplace=True)\n",
    "\n",
    "# Inspect\n",
    "sp500_edgelist_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Visualize the resulting network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "We used TF-IDF vectors of documents and cosine similarities between these document vectors as a way of representing the similarity in the networks above. However, TF-IDF score are simply (normalized) word frequencies: they do not capture semantic information. A vector space model like the popular Word2Vec represents each token (word) in a high-dimensional (here we'll use 100-dimensions) space that is trained from some (ideally) large corpus of documents. Ideally, tokens that are used in similar contexts are placed into similar locations in this high-dimensional space. Once we have vectorized words into this space, we're able to efficiently compute do a variety of other operations such as compute similarities between words or do transformations that can find analogies.\n",
    "\n",
    "I lack the expertise and we lack the time to get into the math behind these methods, but here are some helpful tutorials I've found:\n",
    "* [Word embeddings: exploration, explanation, and exploitation ](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)\n",
    "* [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)\n",
    "* [On word embeddings](http://ruder.io/word-embeddings-1/)\n",
    "* [TensorFlow - Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)\n",
    "\n",
    "We'll use the 44 Presidential biographies as a small and specific corpus. We start by training a `bios_model` from the list of biographies using hyperparamaters for the number of dimensions (size), the number of surrounding words to use as training (window), and the minimum number of times a word has to occur to be included in the model (min_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bios_model = Word2Vec(bios.values(),size=100,window=10,min_count=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the vocabulary exists as a N-dimensional vector, where N is the \"size\" hyper-parameter set in the model. The \"congress\" token in located at this position in the 100-dimensional space we trained in `bios_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv['congress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar('congress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar('court')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar('election')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a `doesnt_match` method that predicts which word in a list doesn't match the other word senses in the list. Sometime the results are predictable/trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.doesnt_match(['democrat','republican','whig','panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other times the results are unexpected/interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.doesnt_match(['canada','mexico','cuba','japan','france'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful implications of having these vectorized embeddings of word meanings is the ability to do operations similar arithmetic that recover or reveal interesting semantic meanings. The classic example is `Man:Woman::King:Queen`:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*keqyBCQ5FL6A7DZLrXamvQ.png)\n",
    "\n",
    "What are some examples of these vector similarities from our trained model?\n",
    "\n",
    "\n",
    "`republican - slavery = democrat - X`\n",
    "\n",
    "`-(republican - slavery) + democrat = X`\n",
    "\n",
    "`slavery + democrat - republican = X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar(positive=['democrat','slavery'],negative=['republican'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.most_similar(positive=['republican','labor'],negative=['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `similarity` method to return the similarity between two terms. In our trained model, \"britain\" and \"france\" are more similar to each other than \"mexico\" and \"canada\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.similarity('republican','democrat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.similarity('mexico','canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_model.wv.similarity('britain','france')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: S&P500 company Word2Vec model\n",
    "\n",
    "**Step 1**: Open the \"sp500_wiki_articles_cleaned.json\" you previous saved of the cleaned S&P500 company article content or use a text preprocessor on \"sp500_wiki_articles.json\" to generate a dictionary of cleaned article content. Train a `sp500_model` using the `Word2Vec` model on the values of the cleaned company article content. You can use default hyperparameters for size, window, and min_count, or experiment with alternative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Using the `most_similar` method, explore some similarities this model has learned for salient tokens about companies (*e.g.*, \"board\", \"controversy\", \"executive\", \"investigation\"). Use the positive and negative options to explore different analogies. Using the `doesnt_match` method, experiment with word combinations to discover predictable and unexpected exceptions. Using the `similarity` method, identify interesting similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Material from this segment is adapted from [Jake Vanderplas](http://jakevdp.github.io/)'s [\"Python Data Science Handbook\" notebooks](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks) and [Kevyn Collins-Thompson](http://www-personal.umich.edu/~kevynct/)'s [\"Applied Machine Learning in Python\"](https://www.coursera.org/learn/python-machine-learning/lecture/XIt7x/introduction) module on Coursera.\n",
    "\n",
    "In the TF-IDF, we have over 17,000 dimensions (corresponding to the unique tokens) for each of the 44 presidential biographies. This data is sparse and large, which makes it hard to visualize. Ideally we'd only have two dimensions of data for a task like visualization.\n",
    "\n",
    "Dimensionality reduction encompasses a set of methods like principal component analysis, multidimensional scaling, and more advanced \"[manifold learning](http://scikit-learn.org/stable/modules/manifold.html#introduction)\" that reduces high-dimensional data down to fewer dimensions. For the purposes of visualization, we typically want 2 dimensions. These methods use a variety of different assumptions and modeling approaches. If you want to understand the differences between them, you'll likely need to find a graduate-level machine learning course. \n",
    "\n",
    "Let's compare what each of these do on our presidential TF-IDF: the goal here is to understand there are different methods for dimensionality reduction and each generates different new components and/or clusters that you'll need to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bio_tfidf_dense.shape)\n",
    "bio_tfidf_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is probably one of the most widely-used and efficient methods for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose a class of models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Step 3: Arrange the data into features matrices\n",
    "# Already done\n",
    "\n",
    "# Step 4: Fit the model to the data\n",
    "pca.fit(bio_tfidf_dense)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "X_pca = pca.transform(bio_tfidf_dense)\n",
    "\n",
    "# Visualize\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1])\n",
    "\n",
    "ax.set_title('PCA')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_pca[i,0],X_pca[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_pca[i,0],X_pca[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_pca[i,0],X_pca[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-dimensional scaling is another common technique in the social sciences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "mds = MDS(n_components=2,metric=False,n_jobs=-1)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_mds = mds.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_mds[:,0],X_mds[:,1])\n",
    "\n",
    "ax.set_title('Multi-Dimensional Scaling')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_mds[i,0],X_mds[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_mds[i,0],X_mds[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_mds[i,0],X_mds[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Isomap](https://en.wikipedia.org/wiki/Isomap) is an extension of MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "iso = Isomap(n_neighbors = 5, n_components = 2)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_iso = iso.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_iso[:,0],X_iso[:,1])\n",
    "\n",
    "ax.set_title('IsoMap')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_iso[i,0],X_iso[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_iso[i,0],X_iso[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_iso[i,0],X_iso[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spectral embedding](https://en.wikipedia.org/wiki/Spectral_clustering) does interesting things to the eigenvectors of a similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "se = SpectralEmbedding(n_components = 2)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_se = se.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(9,6))\n",
    "ax.scatter(X_se[:,0],X_se[:,1])\n",
    "\n",
    "ax.set_title('Spectral Embedding')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_se[i,0],X_se[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_se[i,0],X_se[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_se[i,0],X_se[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Linear Embedding is yet another dimensionality reduction method, but not my favorite to date given performance (meaningful clusters as output) and cost (expensive to compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "lle = LocallyLinearEmbedding(n_components = 2,n_jobs=-1)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_lle = lle.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(9,6))\n",
    "ax.scatter(X_lle[:,0],X_lle[:,1])\n",
    "\n",
    "ax.set_title('Locally Linear Embedding')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_lle[i,0],X_lle[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_lle[i,0],X_lle[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_lle[i,0],X_lle[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[t-Distributed Stochastic Neighbor Embedding](https://lvdmaaten.github.io/tsne/) (t-SNE) is ubiquitous for visualizing word or document embeddings. It can be expensive to run, but it does a great job recovering clusters. There are some hyper-parameters, particularly \"perplexity\" that you'll need to tune to get things to look interesting.\n",
    "\n",
    "Wattenberg, Vi√©gas, and Johnson have an [outstanding interactive tool](https://distill.pub/2016/misread-tsne/) visualizing how t-SNE's different parameters influence the layout as well as good advice on how to make the best of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "tsne = TSNE(n_components = 2, init='pca', random_state=42, perplexity=11)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_tsne = tsne.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_tsne[:,0],X_tsne[:,1])\n",
    "\n",
    "ax.set_title('t-SNE')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_tsne[i,0],X_tsne[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_tsne[i,0],X_tsne[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_tsne[i,0],X_tsne[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Uniform Maniford Approximation and Projection (UMAP)](https://github.com/lmcinnes/umap) is a new and particularly fast dimensionality reduction method with some comparatively great documentation. Unfortunately, UMAP is so new that it hasn't been translated into scikit-learn yet, so you'll need to install it separately from the terminal:\n",
    "\n",
    "`conda install -c conda-forge umap-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "umap_ = UMAP(n_components=2, n_neighbors=10, random_state=42)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_umap = umap_.fit_transform(bio_tfidf_dense)\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_umap[:,0],X_umap[:,1])\n",
    "\n",
    "ax.set_title('UMAP')\n",
    "\n",
    "for i,txt in enumerate(bios.keys()):\n",
    "    if txt == 'Barack Obama':\n",
    "        ax.annotate(txt,(X_umap[i,0],X_umap[i,1]),color='blue',fontweight='bold')\n",
    "    elif txt == 'Donald Trump':\n",
    "        ax.annotate(txt,(X_umap[i,0],X_umap[i,1]),color='red',fontweight='bold')\n",
    "    else:\n",
    "        ax.annotate(txt,(X_umap[i,0],X_umap[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: S&P500 company clusters\n",
    "\n",
    "**Step 1**: Using the `sp500_tfidf_dense` array/DataFrame, experiment with different dimensionality reduction tools we covered above. Visualize and inspect the distribution of S&P500 companies for interesting dimensions (do X and Y dimensions in this reduced data capture anything meaningful?) or clusters (do companies clusters together as we'd expect?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word embeddings\n",
    "\n",
    "Using the `bio_counts`, we can find the top-N most frequent words and save them as `top_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.DataFrame(bio_counts.todense().sum(0).T,\n",
    "                              index=count_vect.get_feature_names())[0]\n",
    "\n",
    "top_words = top_words.sort_values(0,ascending=False).head(1000).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in `top_words`, we get its vector from `bios_model` and add it to the `top_word_vectors` list and cast this list back to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_vectors = []\n",
    "\n",
    "for word in top_words:\n",
    "    try:\n",
    "        vector = bios_model.wv[word]\n",
    "        top_word_vectors.append(vector)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "top_word_vectors = np.array(top_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the dimensionality tools we just covered in the previous section to visualize the word similarities. PCA is fast but rarely does a great job with this extremely high-dimensional and sparse data: it's a cloud of points with no discernable structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "X_w2v = top_word_vectors\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_w2v_pca = pca.fit_transform(X_w2v)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_w2v_pca[:,0],X_w2v_pca[:,1],s=3)\n",
    "\n",
    "ax.set_title('PCA')\n",
    "\n",
    "for i,txt in enumerate(top_words):\n",
    "    if i%10 == 0:\n",
    "        ax.annotate(txt,(X_w2v_pca[i,0],X_w2v_pca[i,1]))\n",
    "        \n",
    "f.savefig('term_pca.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE was more-or-less engineered for precisely the task of visualizing word embeddings. It likely takes on the order of a minute or more for t-SNE to reduce the `top_words` embeddings to only two dimensions. Assuming our perplexity and other t-SNE hyperparameters are well-behaved, there should be relatively easy-to-discern clusters of words with similar meanings. You can also open the \"term_sne.pdf\" file and zoome to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "tsne = TSNE(n_components = 2, init='pca', random_state=42, perplexity=25)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "X_w2v = top_word_vectors\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_w2v_tsne = tsne.fit_transform(X_w2v)\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_w2v_tsne[:,0],X_w2v_tsne[:,1],s=3)\n",
    "\n",
    "ax.set_title('t-SNE')\n",
    "\n",
    "for i,txt in enumerate(top_words):\n",
    "    if i%10 == 0:\n",
    "        ax.annotate(txt,(X_w2v_tsne[i,0],X_w2v_tsne[i,1]))\n",
    "        \n",
    "f.savefig('term_tsne.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP is faster and I think better, but you'll need to make sure this is installed on your system since it doesn't come with scikit-learn or Anaconda by default. Words like \"nominee\" and \"campaign\" or the names of the months cluster clearly together apart from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose your model class(es)\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 2: Instantiate your model class(es)\n",
    "umap_ = UMAP(n_components=2, n_neighbors=5, random_state=42)\n",
    "\n",
    "# Step 3: Arrange data into features matrices\n",
    "# Done!\n",
    "\n",
    "# Step 4: Fit the data and transform\n",
    "X_w2v_umap = umap_.fit_transform(X_w2v)\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(X_w2v_umap[:,0],X_w2v_umap[:,1],s=3)\n",
    "\n",
    "ax.set_title('UMAP')\n",
    "\n",
    "for i,txt in enumerate(top_words):\n",
    "    if i%10 == 0:\n",
    "        ax.annotate(txt,(X_w2v_umap[i,0],X_w2v_umap[i,1]))\n",
    "        \n",
    "f.savefig('term_umap.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Visualizing word embeddings for S&P500 company articles\n",
    "\n",
    "**Step 1**: Compute the word vectors for the top 1000(ish) terms in the S&P500 word counts from your `sp500_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Reduce the dimensionality of these top word vectors using PCA, t-SNE, or (if you've installed it) UMAP and visualize the results. What meaningful or surprising clusters do you discover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
