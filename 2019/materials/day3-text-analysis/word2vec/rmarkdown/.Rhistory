tidy_skipgrams
library(tidyverse)
library(widyr)
tidy_skipgrams <- elected_no_retweets %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))
library(tidytext)
library(dplyr)
# We want to use original tweets, not retweets:
elected_no_retweets <- elected_official_tweets %>%
filter(is_retweet == F) %>%
select(c("text"))
#create tweet id
elected_no_retweets$postID<-row.names(elected_no_retweets)
library(widyr)
tidy_skipgrams <- elected_no_retweets %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
tidy_skipgrams
library(widyr)
#create context window with length 8
tidy_skipgrams <- elected_no_retweets %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize probabilities
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
tidy_skipgrams <- elected_no_retweets %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
#calculate unigram probabilities (used to normalize skipgram probabilities later)
unigram_probs <- elected_no_retweets %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
#normalize probabilities
normalized_prob <- skipgram_probs %>%
filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
normalized_prob
normalized_prob[1000:1005]
normalized_prob[1000:1005,]
normalized_prob[2000:2005,]
normalized_prob[2005:2010,]
library(widyr)
pmi_matrix <- normalized_prob %>%
mutate(pmi = log10(p_together)) %>%
cast_sparse(word1, word2, pmi)
head(pmi_matrix)
head(row.names(pmi_matrix))
head(col.names(pmi_matrix))
head(colnames(pmi_matrix))
pmi_matrix <- normalized_prob %>%
mutate(pmi = log10(p_together)) %>%
cast_sparse(word1, word2, pmi)
library(irlba)
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
#run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
head(word_vectors)
word_vectors[1]
row.names(word_vectors)
search_synonyms <- function(word_vectors, selected_vector) {
similarities <- word_vectors %*% selected_vector %>%
tidy() %>%
as_tibble() %>%
rename(token = .rownames,
similarity = unrowname.x.)
similarities %>%
arrange(-similarity)
}
library(broom)
search_synonyms <- function(word_vectors, selected_vector) {
similarities <- word_vectors %*% selected_vector %>%
tidy() %>%
as_tibble() %>%
rename(token = .rownames,
similarity = unrowname.x.)
similarities %>%
arrange(-similarity)
gop <- search_synonyms(word_vectors,word_vectors["president",])
gop
search_synonyms <- function(word_vectors, selected_vector) {
similarities <- word_vectors %*% selected_vector %>%
tidy() %>%
as_tibble() %>%
rename(token = .rownames,
similarity = unrowname.x.)
similarities %>%
arrange(-similarity)
}
library(broom)
search_synonyms <- function(word_vectors, selected_vector) {
similarities <- word_vectors %*% selected_vector %>%
tidy() %>%
as_tibble() %>%
rename(token = .rownames,
similarity = unrowname.x.)
similarities %>%
arrange(-similarity)
}
gop <- search_synonyms(word_vectors,word_vectors["president",])
gop
gop <- search_synonyms(word_vectors,word_vectors["candidate",])
gop
gop <- search_synonyms(word_vectors,word_vectors["president",])
gop
pmi_svd <- irlba(pmi_matrix, 2, maxit = 500)
#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
#grab first 200 words
forplot<-as.data.frame(word_vectors[1:200,])
forplot$word<-rownames(forplot)
#now plot
library(ggplot2)
ggplot(forplot, aes(x=V1, y=V2, label=word))+
geom_text(aes(label=word),hjust=0, vjust=0)+
theme_minimal()
#grab first 200 words
forplot<-as.data.frame(word_vectors[200:300,])
forplot$word<-rownames(forplot)
ggplot(forplot, aes(x=V1, y=V2, label=word))+
geom_text(aes(label=word),hjust=0, vjust=0)+
theme_minimal()
ggplot(forplot, aes(x=V1, y=V2, label=word))+
geom_text(aes(label=word),hjust=0, vjust=0)+
theme_minimal()+
xlab("First Dimension")+
ylab("Second Dimension")
ggplot(forplot, aes(x=V1, y=V2, label=word))+
geom_text(aes(label=word),hjust=0, vjust=0, color="blue")+
theme_minimal()+
xlab("First Dimension")+
ylab("Second Dimension")
ggplot(forplot, aes(x=V1, y=V2, label=word))+
geom_text(aes(label=word),hjust=0, vjust=0, color="blue")+
theme_minimal()+
xlab("First Dimension Created by SVD")+
ylab("Second Dimension Created by SVD")
