
<style>
.reveal section p {
  color: black;
  font-size: .7em;
  font-family: 'Helvetica'; #this is the font/color of text in slides
}


.section .reveal .state-background {
    background: white;}
.section .reveal h1,
.section .reveal p {
    color: black;
    position: relative;
    top: 4%;}

.wrap-url pre code {
  word-wrap:break-word;
}

</style>


Part of Speech Tagging, Sentence Parsing, and Named-Entity Recognition
========================================================
author: Chris Bail 
date: Duke University
autosize: true
transition: fade  
  website: https://www.chrisbail.net  
  github: https://github.com/cbail  
  Twitter: https://www.twitter.com/chris_bail


Outline For Today
========================================================
&nbsp;

1) Part of Speech Tagging;  
2) Sentence Parsing;  
3) Named-Entity Recognition

========================================================

## **Part of Speech Tagging**



What is Part of Speech (POS) Tagging?
========================================================

Examples from the Penn Tree Bank
========================================================
&nbsp;

<img src="examplestreebank.png" height="400" />

What is the the Penn Tree Bank?
========================================================
&nbsp;

Hand-coded POS annotations for:

1) 1 Million words of 1989 Wall Street Journal Material  
2) Fully-tagged version of the Brown Corpus (~ 1 mill works published in U.S. since 1961)  
3) Other samples



How does POS-Tagging work?
========================================================




Backend Options for POS Tagging
========================================================
&nbsp;

Stanford CoreNLP   
NLTK Module (in Python)  
UDPipe (new)


POS Tagging in CleanNLP
========================================================
&nbsp;

CleanNLP is a relatively new package in R that handles a variety of NLP tasks including POS tagging. It supports a variety of different backends for this purpose, including CoreNLP, Spacy, and udpipe. We are going to use udpipe because it does not require Java or Python to be installed.  

Here's how we initialize cleanNLP with udpipe:

```{r}
library(cleanNLP)
#cnlp_download_udpipe()
cnlp_init_udpipe()
```


POS Tagging in CleanNLP
========================================================
class: wrap-url
&nbsp;
Next, let's conduct part of speech tagging on the first tweet from our elected official twitter data:
```{r}
load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))
elected_official_tweets$text[1]
```

POS Tagging in CleanNLP
========================================================
&nbsp;

First, we annotate the sentence, then we use the `cnlp_get_token` function to extract the part of speech tags for the first 10 words in the first sentence:
```{r}
annotation <- cnlp_annotate(elected_official_tweets$text[1])
pos<-cnlp_get_token(annotation)
head(pos, 10)
```

POS Tagging is Highly Accurate...But
========================================================
&nbsp;

POS-tagging assumes the language is reasonably well constructed... and this is not always the case on Twitter:

```{r}
elected_official_tweets$text[4403]
```

POS Tagging is Highly Accurate...But
========================================================
&nbsp;
```{r}
annotation <- cnlp_annotate(elected_official_tweets$text[4403])
pos<-cnlp_get_token(annotation)
head(pos, 10)
```


========================================================

## **Sentence Parsing**



What is Sentence Parsing?
========================================================
&nbsp;  
Sentence parsing refers to a technique used to parse sentences into their constituent parts in terms of their syntactic relation to each other.


What is Sentence Parsing?
========================================================
&nbsp;

<img src="parse.png" height="400" />

Sentence Parsing in CleanNLP
========================================================
&nbsp;

"Mortally wounded, Ari Fuld heroically pursued the Palestinian terrorist"

```{r}
annotation <- cnlp_annotate(elected_official_tweets$text[1])
dependency<-cnlp_get_dependency(annotation)
head(dependency, 10)
```

Why do we need Sentence Parsing?
========================================================
%nbsp; 

Useful for extracting information from texts about named entities as well as relationships between entities mentioned within texts

========================================================

## **Named-Entity Recognition**



Named Entity Recognition
========================================================
%nbsp; 
Named entity recognition is not supported by the udpipe backend, so we need to switch to the coreNLP backend. This often raises very annoying Java issues-- particularly on OSX-- be warned! 

In most cases, you will need to install one of the latest versions of the
[java development kit](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). You may also need to run this code to link the JDK to your version of R: 

```{r, eval=FALSE}
<<<<<<< Updated upstream
sudo R CMD javareconf
```



Named Entity Recognition
========================================================
%nbsp; 

Once we have dealt with the Java issues, we can download the coreNLP program (this will take a while, unfortunately):
```{r, eval=FALSE}
=======
>>>>>>> Stashed changes
cnlp_download_corenlp()
```


Named Entity Recognition
========================================================
%nbsp;

Finally, we can extract named entities as follows:

```{r, eval=FALSE}
cnlp_init_corenlp()
annotation <- cnlp_annotate(elected_official_tweets$text[1])
entitities<-cnlp_get_entity(annotation)
entitities
```


Entities available via coreNLP
========================================================
%nbsp;  

the Stanford coreNLP can identify the following types of entities:

Dates  
Locations  
Money  
Organization  
Percent  
Person  
Time  



========================================================
%nbsp;  

## **Now YOU Try it**

1) Extract 100 random documents/texts from a text-based dataset  
2) Identify all of the proper nouns (or named entities) in each document  
3) Identify the adjectives that moderated the proper nouns or named entities  
4) Run a sentiment analysis on those adjective modifiers  
5) Produce a visual of the propoer nouns or named entities with the most positive and most negative sentiment  

========================================================
