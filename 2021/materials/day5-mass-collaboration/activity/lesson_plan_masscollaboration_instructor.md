# Lesson Plan for Instructors
## Summer Institutes in Computational Social Science 2021
## Day 5, Mass collaboration
## Prepared by Emily Cantrell and Matt Salganik

This document assumes that you have also read the [lesson plan for participants](https://github.com/compsocialscience/summer-institute/blob/master/2021/materials/day5-mass-collaboration/activity/lesson_plan_masscollaboration_participant.md).

### Instructor preparation

Well in advance of the activity, participants will need to apply for data access. Please reach out to the Fragile Families and Child Wellbeing Study team (ffdata@princeton.edu) a few weeks in advance to check on their availabilty to approve data access. You should require your participants to apply for data access at least a few days in advance of the activity, or possibly sooner if the FFCWS team has limited availability the week of your activity. We recommend that you apply for data access yourself as soon as possible so that you can help participants if they have questions. 

Familiarize yourself with the data files after you receive the ones we will share with you through the Secure Send system. They include components to support participants who may be falling behind, including a pre-prepared data file.

### Ethics: Guidelines for instructors

Remind your participants of the importance of using the data ethically. The reason projects like the Fragile Families Challenge are possible is because those who share their data trust that researchers will use it appropriately. As the local organizer, you have some power to set norms about handling data safely, not seeking to identify any individuals, and deleting the data when the analysis is complete.  Please remind participants to delete their data files at the end of the activity.

### Group formation

Form groups of roughly 3 people randomly. You might consider distributing those with the most predictive modeling experience across groups.

### A note about scoring

The [online submission site](https://codalab.fragilefamilieschallenge.org/competitions/28) scores submissions on the leaderboard set (530 cases, 1/8 of the sample). Participants are able to view their score on this set for each submission. In the original Challenge, we ultimately scored participants one time on the holdout set (1,591 cases). Scoring with the holdout set is unavailable for SICSS activities. For pedagogical purposes, it suffices for participants at SICSS to see their score on the leaderboard set, which will happen automatically in real time.

### How to debrief

In the debrief, here are some possible starter questions:

- How did you go about preparing the data file?
- Were there unexpected hurdles when preparing the data file? How did you address them? It is likely that groups took a range of approaches.
- How did you go about the predictive modeling exercise?
- What aspects of the exercise were successful from the standpoint of a participant?
- If you were organizing this mass collaboration, what would you change to make the experience better?
- Are there other questions we might tackle more successfully by working together than by working alone?

### Notes

- Many participants struggle with data preparation. Learning about data preparation is one of the learning objectives of the activity, but if you want to skip that part, you can have them work with the data file that we have already prepared.
- The schedule that we proposed for participants is modular: before lunch prepare your data, and after lunch build your model and submit to the leaderboard. Many participants really enjoy uploading predictions to the leaderboard and seeing their score, so if they seem to struggle to get the data prepared before lunch, you can encourage them to switch to working with the data file that we have already prepared.
